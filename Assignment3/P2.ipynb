{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mobashera-Alam/CS-5783-MachineLearning/blob/main/Assignment3/P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBSIoxQT8etJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBEimwOY8t0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d1da37-4eed-4058-93a9-c96970f1dd0b"
      },
      "source": [
        "# loading the dataset\n",
        "\n",
        "(X_train, y_train) , (X_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GodiyqgeL7aT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "cad1bf6d-a8a9-4bee-913c-50927b2e0159"
      },
      "source": [
        "#randomly printing one image for data visualization\n",
        "i = random.randint(1, 50000)\n",
        "plt.imshow(X_train[i]) # showing the image\n",
        "y_train[i] # showing the label of that images"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XulMFIqVOIIv"
      },
      "source": [
        "# Coverting the images to gray image (32X32X1) from colored format (32X32X3)\n",
        "X_train_gray = np.sum(X_train/3, axis=3, keepdims = True)\n",
        "X_test_gray = np.sum(X_test/3, axis=3, keepdims = True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtKynqKnOX6H"
      },
      "source": [
        "#Data Standardization\n",
        "X_train_gray_norm = (X_train_gray - 128) / 128\n",
        "X_test_gray_norm = (X_test_gray - 128) / 128"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoufgKpAPVkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86072eb-c60a-4353-b073-facb6f986d4b"
      },
      "source": [
        "X_train_gray_norm.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Given Model\n",
        "\n",
        "LeNet = models.Sequential()\n",
        "\n",
        "LeNet.add(layers.Conv2D(6, (5,5), strides = (1,1), activation = 'relu', input_shape= (32,32,1)))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "\n",
        "LeNet.add(layers.Conv2D(16, (5,5), strides=(1,1),activation = 'relu'))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "LeNet.add(layers.Conv2D(120,(5,5), activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Flatten())\n",
        "\n",
        "\n",
        "\n",
        "LeNet.add(layers.Dense(84, activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Dense(10, activation='softmax'))\n",
        "LeNet.summary()"
      ],
      "metadata": {
        "id": "kEI5u88AUeAd",
        "outputId": "acc61091-3dc8-4429-b3a6-f03f62f3bc6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_14 (Conv2D)          (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 14, 14, 6)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 5, 5, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-KmJqtxPa5f"
      },
      "source": [
        "adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "LeNet.compile(optimizer = adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxuuoe1jPeK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab9574b-93e7-4658-e770-a23d53cc9b2d"
      },
      "source": [
        "history = LeNet.fit(X_train_gray_norm, y_train, epochs=25, batch_size=200, validation_split=0.2, verbose=1)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "200/200 [==============================] - 2s 6ms/step - loss: 2.0626 - accuracy: 0.2483 - val_loss: 1.9129 - val_accuracy: 0.3017\n",
            "Epoch 2/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8848 - accuracy: 0.3088 - val_loss: 1.8377 - val_accuracy: 0.3223\n",
            "Epoch 3/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8099 - accuracy: 0.3360 - val_loss: 1.7836 - val_accuracy: 0.3465\n",
            "Epoch 4/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7703 - accuracy: 0.3523 - val_loss: 1.7571 - val_accuracy: 0.3547\n",
            "Epoch 5/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7542 - accuracy: 0.3600 - val_loss: 1.7357 - val_accuracy: 0.3684\n",
            "Epoch 6/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7270 - accuracy: 0.3698 - val_loss: 1.7252 - val_accuracy: 0.3735\n",
            "Epoch 7/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7191 - accuracy: 0.3746 - val_loss: 1.7201 - val_accuracy: 0.3715\n",
            "Epoch 8/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7068 - accuracy: 0.3803 - val_loss: 1.7105 - val_accuracy: 0.3794\n",
            "Epoch 9/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6960 - accuracy: 0.3832 - val_loss: 1.6972 - val_accuracy: 0.3769\n",
            "Epoch 10/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6867 - accuracy: 0.3883 - val_loss: 1.7166 - val_accuracy: 0.3712\n",
            "Epoch 11/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6766 - accuracy: 0.3868 - val_loss: 1.7000 - val_accuracy: 0.3761\n",
            "Epoch 12/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6619 - accuracy: 0.3967 - val_loss: 1.6865 - val_accuracy: 0.3795\n",
            "Epoch 13/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6620 - accuracy: 0.3936 - val_loss: 1.6785 - val_accuracy: 0.3796\n",
            "Epoch 14/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6636 - accuracy: 0.3974 - val_loss: 1.6913 - val_accuracy: 0.3862\n",
            "Epoch 15/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6518 - accuracy: 0.3990 - val_loss: 1.6680 - val_accuracy: 0.3917\n",
            "Epoch 16/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6414 - accuracy: 0.4027 - val_loss: 1.6745 - val_accuracy: 0.3937\n",
            "Epoch 17/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6410 - accuracy: 0.4025 - val_loss: 1.6737 - val_accuracy: 0.3909\n",
            "Epoch 18/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6270 - accuracy: 0.4106 - val_loss: 1.6456 - val_accuracy: 0.4019\n",
            "Epoch 19/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6026 - accuracy: 0.4171 - val_loss: 1.6076 - val_accuracy: 0.4168\n",
            "Epoch 20/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5794 - accuracy: 0.4254 - val_loss: 1.5843 - val_accuracy: 0.4254\n",
            "Epoch 21/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5633 - accuracy: 0.4335 - val_loss: 1.5862 - val_accuracy: 0.4257\n",
            "Epoch 22/25\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.5636 - accuracy: 0.4322 - val_loss: 1.6457 - val_accuracy: 0.3987\n",
            "Epoch 23/25\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.5400 - accuracy: 0.4428 - val_loss: 1.6040 - val_accuracy: 0.4193\n",
            "Epoch 24/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5445 - accuracy: 0.4393 - val_loss: 1.6013 - val_accuracy: 0.4243\n",
            "Epoch 25/25\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.5393 - accuracy: 0.4466 - val_loss: 1.6064 - val_accuracy: 0.4251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import datasets, layers, models\n",
        "LeNet = models.Sequential()\n",
        "\n",
        "LeNet.add(layers.Conv2D(6, (5,5), strides = (1,1), activation = 'relu', input_shape= (32,32,1)))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "\n",
        "LeNet.add(layers.Conv2D(16, (5,5), strides=(1,1),activation = 'relu'))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "LeNet.add(layers.Conv2D(120,(5,5), activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Flatten())\n",
        "\n",
        "\n",
        "\n",
        "LeNet.add(layers.Dense(84, activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Dense(10, activation='softmax'))\n",
        "LeNet.summary()\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "LeNet.compile(optimizer = adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v8UxxZA54DB",
        "outputId": "21bad4b7-9112-4287-b494-76f9c85fbac0"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_45 (Conv2D)          (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " max_pooling2d_20 (MaxPoolin  (None, 14, 14, 6)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_46 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " max_pooling2d_21 (MaxPoolin  (None, 5, 5, 16)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_47 (Conv2D)          (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 120)               0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = LeNet.fit(X_train_gray_norm, y_train, epochs=25, batch_size=200, validation_split=0.2, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-2fHl1G6Bsf",
        "outputId": "d80722ca-219e-4201-de7e-de412b8b6421"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "200/200 [==============================] - 2s 6ms/step - loss: 2.1464 - accuracy: 0.2240 - val_loss: 2.0108 - val_accuracy: 0.2820\n",
            "Epoch 2/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9372 - accuracy: 0.3103 - val_loss: 1.8655 - val_accuracy: 0.3435\n",
            "Epoch 3/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8003 - accuracy: 0.3606 - val_loss: 1.7456 - val_accuracy: 0.3774\n",
            "Epoch 4/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7103 - accuracy: 0.3921 - val_loss: 1.6810 - val_accuracy: 0.4029\n",
            "Epoch 5/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6628 - accuracy: 0.4086 - val_loss: 1.6491 - val_accuracy: 0.4078\n",
            "Epoch 6/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6330 - accuracy: 0.4195 - val_loss: 1.6208 - val_accuracy: 0.4212\n",
            "Epoch 7/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6059 - accuracy: 0.4278 - val_loss: 1.6049 - val_accuracy: 0.4248\n",
            "Epoch 8/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5840 - accuracy: 0.4386 - val_loss: 1.5847 - val_accuracy: 0.4363\n",
            "Epoch 9/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5628 - accuracy: 0.4444 - val_loss: 1.5629 - val_accuracy: 0.4482\n",
            "Epoch 10/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5449 - accuracy: 0.4542 - val_loss: 1.5501 - val_accuracy: 0.4520\n",
            "Epoch 11/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5283 - accuracy: 0.4604 - val_loss: 1.5370 - val_accuracy: 0.4557\n",
            "Epoch 12/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5123 - accuracy: 0.4668 - val_loss: 1.5248 - val_accuracy: 0.4607\n",
            "Epoch 13/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4954 - accuracy: 0.4753 - val_loss: 1.5125 - val_accuracy: 0.4643\n",
            "Epoch 14/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4808 - accuracy: 0.4780 - val_loss: 1.4956 - val_accuracy: 0.4708\n",
            "Epoch 15/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4668 - accuracy: 0.4839 - val_loss: 1.4801 - val_accuracy: 0.4761\n",
            "Epoch 16/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4563 - accuracy: 0.4880 - val_loss: 1.4685 - val_accuracy: 0.4830\n",
            "Epoch 17/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4441 - accuracy: 0.4918 - val_loss: 1.4630 - val_accuracy: 0.4872\n",
            "Epoch 18/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4312 - accuracy: 0.4966 - val_loss: 1.4572 - val_accuracy: 0.4887\n",
            "Epoch 19/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4226 - accuracy: 0.5004 - val_loss: 1.4453 - val_accuracy: 0.4961\n",
            "Epoch 20/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4105 - accuracy: 0.5050 - val_loss: 1.4361 - val_accuracy: 0.4998\n",
            "Epoch 21/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4052 - accuracy: 0.5048 - val_loss: 1.4323 - val_accuracy: 0.5036\n",
            "Epoch 22/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3951 - accuracy: 0.5100 - val_loss: 1.4196 - val_accuracy: 0.5003\n",
            "Epoch 23/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3843 - accuracy: 0.5138 - val_loss: 1.4118 - val_accuracy: 0.5034\n",
            "Epoch 24/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3764 - accuracy: 0.5167 - val_loss: 1.4049 - val_accuracy: 0.5084\n",
            "Epoch 25/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3701 - accuracy: 0.5189 - val_loss: 1.4042 - val_accuracy: 0.5100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import datasets, layers, models\n",
        "LeNet = models.Sequential()\n",
        "\n",
        "LeNet.add(layers.Conv2D(6, (5,5), strides = (1,1), activation = 'relu', input_shape= (32,32,1)))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "\n",
        "LeNet.add(layers.Conv2D(16, (5,5), strides=(1,1),activation = 'relu'))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "LeNet.add(layers.Conv2D(120,(5,5), activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Flatten())\n",
        "\n",
        "\n",
        "\n",
        "LeNet.add(layers.Dense(84, activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Dense(10, activation='softmax'))\n",
        "LeNet.summary()\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=100)\n",
        "LeNet.compile(optimizer = adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rafNESDI6pr0",
        "outputId": "bb87d69d-46b7-4c59-c448-7e651d9fec3a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_48 (Conv2D)          (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " max_pooling2d_22 (MaxPoolin  (None, 14, 14, 6)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_49 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " max_pooling2d_23 (MaxPoolin  (None, 5, 5, 16)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_50 (Conv2D)          (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 120)               0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = LeNet.fit(X_train_gray_norm, y_train, epochs=25, batch_size=200, validation_split=0.2, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVq-zmKP6wXi",
        "outputId": "1ddb1d3a-6909-4306-9532-1892c6f336c0"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "200/200 [==============================] - 2s 6ms/step - loss: 905480843034624.0000 - accuracy: 0.1016 - val_loss: 45.1249 - val_accuracy: 0.1025\n",
            "Epoch 2/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 51.5726 - accuracy: 0.0992 - val_loss: 44.1118 - val_accuracy: 0.0980\n",
            "Epoch 3/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 48.3142 - accuracy: 0.1013 - val_loss: 55.0616 - val_accuracy: 0.1016\n",
            "Epoch 4/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 55.8442 - accuracy: 0.1016 - val_loss: 60.4897 - val_accuracy: 0.1014\n",
            "Epoch 5/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 53.4842 - accuracy: 0.1012 - val_loss: 78.7622 - val_accuracy: 0.1014\n",
            "Epoch 6/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 52.7945 - accuracy: 0.0981 - val_loss: 45.9419 - val_accuracy: 0.1025\n",
            "Epoch 7/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 48.7112 - accuracy: 0.1001 - val_loss: 47.3051 - val_accuracy: 0.0980\n",
            "Epoch 8/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 55.5275 - accuracy: 0.1010 - val_loss: 44.1578 - val_accuracy: 0.1022\n",
            "Epoch 9/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 48.4124 - accuracy: 0.1004 - val_loss: 52.4040 - val_accuracy: 0.1016\n",
            "Epoch 10/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 46.6877 - accuracy: 0.0992 - val_loss: 40.1822 - val_accuracy: 0.1022\n",
            "Epoch 11/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 48.4121 - accuracy: 0.1015 - val_loss: 39.0377 - val_accuracy: 0.1016\n",
            "Epoch 12/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 50.8671 - accuracy: 0.1037 - val_loss: 61.4690 - val_accuracy: 0.1022\n",
            "Epoch 13/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 54.0823 - accuracy: 0.1006 - val_loss: 63.2642 - val_accuracy: 0.0997\n",
            "Epoch 14/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 54.7556 - accuracy: 0.0975 - val_loss: 23.6947 - val_accuracy: 0.0997\n",
            "Epoch 15/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 59.6885 - accuracy: 0.1000 - val_loss: 59.9753 - val_accuracy: 0.1016\n",
            "Epoch 16/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 47.6392 - accuracy: 0.0983 - val_loss: 48.6115 - val_accuracy: 0.1016\n",
            "Epoch 17/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 56.5644 - accuracy: 0.1010 - val_loss: 32.3138 - val_accuracy: 0.1025\n",
            "Epoch 18/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 54.1530 - accuracy: 0.1001 - val_loss: 65.3546 - val_accuracy: 0.1022\n",
            "Epoch 19/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 51.6586 - accuracy: 0.0991 - val_loss: 37.9695 - val_accuracy: 0.1016\n",
            "Epoch 20/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 49.1462 - accuracy: 0.1003 - val_loss: 62.4747 - val_accuracy: 0.0977\n",
            "Epoch 21/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 60.0365 - accuracy: 0.1009 - val_loss: 64.3850 - val_accuracy: 0.0952\n",
            "Epoch 22/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 50.8724 - accuracy: 0.0975 - val_loss: 54.6494 - val_accuracy: 0.1014\n",
            "Epoch 23/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 49.8249 - accuracy: 0.1004 - val_loss: 49.8364 - val_accuracy: 0.1025\n",
            "Epoch 24/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 47.2081 - accuracy: 0.1007 - val_loss: 36.4770 - val_accuracy: 0.0980\n",
            "Epoch 25/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 51.2965 - accuracy: 0.1009 - val_loss: 38.7159 - val_accuracy: 0.1014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the given number of epoch and fixed batch size,we tested three different learning rate = 0.0001,0.01,10. Based on our study the model accuracy learning rate=0.0001 works best in training set (51%) than the other two. Learning rate is important for obtaining the global minima, very high learning rate can cause fluctuation between the local minima. On the other hand too low learning rate can results is very slow progress towards the global minima."
      ],
      "metadata": {
        "id": "Ednp1YA56-QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Various Batch size=500\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "LeNet = models.Sequential()\n",
        "\n",
        "LeNet.add(layers.Conv2D(6, (5,5), strides = (1,1), activation = 'relu', input_shape= (32,32,1)))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "\n",
        "LeNet.add(layers.Conv2D(16, (5,5), strides=(1,1),activation = 'relu'))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "LeNet.add(layers.Conv2D(120,(5,5), activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Flatten())\n",
        "\n",
        "\n",
        "\n",
        "LeNet.add(layers.Dense(84, activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Dense(10, activation='softmax'))\n",
        "LeNet.summary()\n",
        "LeNet.compile(optimizer = 'Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = LeNet.fit(X_train_gray_norm, y_train, epochs=25, batch_size=500, validation_split=0.2, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB5M2L-o9o8Q",
        "outputId": "9647b850-bc3f-48c7-a253-67d536970506"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 1.1932 - accuracy: 0.5761 - val_loss: 1.4045 - val_accuracy: 0.5101\n",
            "Epoch 2/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1861 - accuracy: 0.5789 - val_loss: 1.4033 - val_accuracy: 0.5089\n",
            "Epoch 3/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1816 - accuracy: 0.5778 - val_loss: 1.4019 - val_accuracy: 0.5115\n",
            "Epoch 4/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1778 - accuracy: 0.5810 - val_loss: 1.4003 - val_accuracy: 0.5127\n",
            "Epoch 5/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1726 - accuracy: 0.5824 - val_loss: 1.4033 - val_accuracy: 0.5138\n",
            "Epoch 6/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1686 - accuracy: 0.5839 - val_loss: 1.4002 - val_accuracy: 0.5143\n",
            "Epoch 7/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1662 - accuracy: 0.5845 - val_loss: 1.4016 - val_accuracy: 0.5130\n",
            "Epoch 8/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1634 - accuracy: 0.5860 - val_loss: 1.3964 - val_accuracy: 0.5125\n",
            "Epoch 9/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1575 - accuracy: 0.5887 - val_loss: 1.4018 - val_accuracy: 0.5164\n",
            "Epoch 10/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1558 - accuracy: 0.5884 - val_loss: 1.3995 - val_accuracy: 0.5150\n",
            "Epoch 11/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1536 - accuracy: 0.5906 - val_loss: 1.3989 - val_accuracy: 0.5153\n",
            "Epoch 12/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1464 - accuracy: 0.5932 - val_loss: 1.4038 - val_accuracy: 0.5125\n",
            "Epoch 13/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1443 - accuracy: 0.5939 - val_loss: 1.3958 - val_accuracy: 0.5169\n",
            "Epoch 14/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1406 - accuracy: 0.5964 - val_loss: 1.4156 - val_accuracy: 0.5113\n",
            "Epoch 15/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1369 - accuracy: 0.5975 - val_loss: 1.4000 - val_accuracy: 0.5207\n",
            "Epoch 16/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1325 - accuracy: 0.5997 - val_loss: 1.4016 - val_accuracy: 0.5187\n",
            "Epoch 17/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1305 - accuracy: 0.5994 - val_loss: 1.4005 - val_accuracy: 0.5149\n",
            "Epoch 18/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1267 - accuracy: 0.6011 - val_loss: 1.4007 - val_accuracy: 0.5173\n",
            "Epoch 19/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1240 - accuracy: 0.6017 - val_loss: 1.4062 - val_accuracy: 0.5150\n",
            "Epoch 20/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1225 - accuracy: 0.6015 - val_loss: 1.4139 - val_accuracy: 0.5132\n",
            "Epoch 21/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1168 - accuracy: 0.6048 - val_loss: 1.4066 - val_accuracy: 0.5160\n",
            "Epoch 22/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1148 - accuracy: 0.6042 - val_loss: 1.4056 - val_accuracy: 0.5167\n",
            "Epoch 23/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1101 - accuracy: 0.6072 - val_loss: 1.4115 - val_accuracy: 0.5173\n",
            "Epoch 24/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1077 - accuracy: 0.6076 - val_loss: 1.4088 - val_accuracy: 0.5147\n",
            "Epoch 25/25\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1057 - accuracy: 0.6070 - val_loss: 1.4077 - val_accuracy: 0.5168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Various Batch size= 20\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "LeNet = models.Sequential()\n",
        "\n",
        "LeNet.add(layers.Conv2D(6, (5,5), strides = (1,1), activation = 'relu', input_shape= (32,32,1)))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "\n",
        "LeNet.add(layers.Conv2D(16, (5,5), strides=(1,1),activation = 'relu'))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "LeNet.add(layers.Conv2D(120,(5,5), activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Flatten())\n",
        "\n",
        "\n",
        "\n",
        "LeNet.add(layers.Dense(84, activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Dense(10, activation='softmax'))\n",
        "LeNet.summary()\n",
        "LeNet.compile(optimizer = 'Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = LeNet.fit(X_train_gray_norm, y_train, epochs=25, batch_size=20, validation_split=0.2, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzyLiWUhBCXp",
        "outputId": "da5b7560-1f6f-4dd5-d352-2daf751a04b3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_17 (Conv2D)          (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 14, 14, 6)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 5, 5, 16)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1.6550 - accuracy: 0.4056 - val_loss: 1.4332 - val_accuracy: 0.4884\n",
            "Epoch 2/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1.3539 - accuracy: 0.5197 - val_loss: 1.3079 - val_accuracy: 0.5344\n",
            "Epoch 3/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1.2247 - accuracy: 0.5679 - val_loss: 1.2349 - val_accuracy: 0.5651\n",
            "Epoch 4/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1.1387 - accuracy: 0.5983 - val_loss: 1.2016 - val_accuracy: 0.5820\n",
            "Epoch 5/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1.0675 - accuracy: 0.6249 - val_loss: 1.1861 - val_accuracy: 0.5886\n",
            "Epoch 6/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1.0112 - accuracy: 0.6454 - val_loss: 1.2098 - val_accuracy: 0.5790\n",
            "Epoch 7/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.9639 - accuracy: 0.6603 - val_loss: 1.1559 - val_accuracy: 0.6010\n",
            "Epoch 8/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.9151 - accuracy: 0.6788 - val_loss: 1.1245 - val_accuracy: 0.6111\n",
            "Epoch 9/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8769 - accuracy: 0.6905 - val_loss: 1.1687 - val_accuracy: 0.6068\n",
            "Epoch 10/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.8361 - accuracy: 0.7049 - val_loss: 1.1672 - val_accuracy: 0.6097\n",
            "Epoch 11/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.8027 - accuracy: 0.7164 - val_loss: 1.1881 - val_accuracy: 0.6040\n",
            "Epoch 12/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.7629 - accuracy: 0.7315 - val_loss: 1.1919 - val_accuracy: 0.6092\n",
            "Epoch 13/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.7344 - accuracy: 0.7390 - val_loss: 1.2333 - val_accuracy: 0.6009\n",
            "Epoch 14/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.7049 - accuracy: 0.7507 - val_loss: 1.2491 - val_accuracy: 0.6066\n",
            "Epoch 15/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.6758 - accuracy: 0.7619 - val_loss: 1.3063 - val_accuracy: 0.5982\n",
            "Epoch 16/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.6520 - accuracy: 0.7673 - val_loss: 1.3153 - val_accuracy: 0.6092\n",
            "Epoch 17/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.6258 - accuracy: 0.7790 - val_loss: 1.3905 - val_accuracy: 0.5984\n",
            "Epoch 18/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.6047 - accuracy: 0.7868 - val_loss: 1.4106 - val_accuracy: 0.5985\n",
            "Epoch 19/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.5867 - accuracy: 0.7918 - val_loss: 1.4378 - val_accuracy: 0.5912\n",
            "Epoch 20/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5643 - accuracy: 0.8008 - val_loss: 1.4922 - val_accuracy: 0.5923\n",
            "Epoch 21/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5456 - accuracy: 0.8062 - val_loss: 1.5301 - val_accuracy: 0.5834\n",
            "Epoch 22/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5190 - accuracy: 0.8161 - val_loss: 1.6515 - val_accuracy: 0.5864\n",
            "Epoch 23/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5107 - accuracy: 0.8167 - val_loss: 1.6181 - val_accuracy: 0.5848\n",
            "Epoch 24/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4932 - accuracy: 0.8235 - val_loss: 1.6833 - val_accuracy: 0.5845\n",
            "Epoch 25/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4808 - accuracy: 0.8271 - val_loss: 1.6704 - val_accuracy: 0.5857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch size=200\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "LeNet = models.Sequential()\n",
        "\n",
        "LeNet.add(layers.Conv2D(6, (5,5), strides = (1,1), activation = 'relu', input_shape= (32,32,1)))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "\n",
        "LeNet.add(layers.Conv2D(16, (5,5), strides=(1,1),activation = 'relu'))\n",
        "LeNet.add(layers.MaxPooling2D(pool_size=2,strides=(2,2)))\n",
        "LeNet.add(layers.Conv2D(120,(5,5), activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Flatten())\n",
        "\n",
        "\n",
        "\n",
        "LeNet.add(layers.Dense(84, activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Dense(10, activation='softmax'))\n",
        "LeNet.summary()\n",
        "LeNet.compile(optimizer = 'Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = LeNet.fit(X_train_gray_norm, y_train, epochs=25, batch_size=200, validation_split=0.2, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyNvAYVtDlfM",
        "outputId": "ede1911e-a692-4695-d70f-865cc7ac5476"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_20 (Conv2D)          (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 14, 14, 6)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  (None, 5, 5, 16)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "200/200 [==============================] - 2s 6ms/step - loss: 1.8596 - accuracy: 0.3297 - val_loss: 1.6731 - val_accuracy: 0.4029\n",
            "Epoch 2/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6031 - accuracy: 0.4272 - val_loss: 1.5394 - val_accuracy: 0.4460\n",
            "Epoch 3/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4888 - accuracy: 0.4727 - val_loss: 1.4468 - val_accuracy: 0.4868\n",
            "Epoch 4/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4022 - accuracy: 0.5055 - val_loss: 1.3884 - val_accuracy: 0.5175\n",
            "Epoch 5/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3420 - accuracy: 0.5264 - val_loss: 1.3440 - val_accuracy: 0.5252\n",
            "Epoch 6/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2852 - accuracy: 0.5490 - val_loss: 1.3101 - val_accuracy: 0.5400\n",
            "Epoch 7/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2483 - accuracy: 0.5646 - val_loss: 1.2620 - val_accuracy: 0.5595\n",
            "Epoch 8/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2056 - accuracy: 0.5798 - val_loss: 1.2571 - val_accuracy: 0.5617\n",
            "Epoch 9/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1800 - accuracy: 0.5877 - val_loss: 1.2464 - val_accuracy: 0.5661\n",
            "Epoch 10/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1507 - accuracy: 0.6005 - val_loss: 1.2275 - val_accuracy: 0.5745\n",
            "Epoch 11/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1201 - accuracy: 0.6120 - val_loss: 1.1939 - val_accuracy: 0.5811\n",
            "Epoch 12/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1006 - accuracy: 0.6175 - val_loss: 1.1749 - val_accuracy: 0.5881\n",
            "Epoch 13/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0740 - accuracy: 0.6247 - val_loss: 1.1748 - val_accuracy: 0.5904\n",
            "Epoch 14/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0488 - accuracy: 0.6376 - val_loss: 1.1618 - val_accuracy: 0.5961\n",
            "Epoch 15/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0325 - accuracy: 0.6406 - val_loss: 1.1465 - val_accuracy: 0.6052\n",
            "Epoch 16/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0114 - accuracy: 0.6520 - val_loss: 1.1461 - val_accuracy: 0.6028\n",
            "Epoch 17/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9908 - accuracy: 0.6579 - val_loss: 1.1284 - val_accuracy: 0.6086\n",
            "Epoch 18/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9744 - accuracy: 0.6600 - val_loss: 1.1325 - val_accuracy: 0.6092\n",
            "Epoch 19/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9589 - accuracy: 0.6677 - val_loss: 1.1190 - val_accuracy: 0.6093\n",
            "Epoch 20/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9482 - accuracy: 0.6696 - val_loss: 1.1690 - val_accuracy: 0.5988\n",
            "Epoch 21/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9235 - accuracy: 0.6815 - val_loss: 1.1138 - val_accuracy: 0.6139\n",
            "Epoch 22/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9085 - accuracy: 0.6852 - val_loss: 1.1464 - val_accuracy: 0.6155\n",
            "Epoch 23/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8934 - accuracy: 0.6879 - val_loss: 1.1296 - val_accuracy: 0.6093\n",
            "Epoch 24/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8747 - accuracy: 0.6960 - val_loss: 1.1267 - val_accuracy: 0.6172\n",
            "Epoch 25/25\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8629 - accuracy: 0.6999 - val_loss: 1.1325 - val_accuracy: 0.6144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyXMo1xAPtTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda848c4-1f97-44d9-de9e-2e353f3ad455"
      },
      "source": [
        "score = LeNet.evaluate(X_test_gray_norm, y_test)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1347 - accuracy: 0.6132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Three different batch size is tested = 20,200,500. Based on the training and validation set result, very low batch size can result in overfitting in the training set. On the contrary too large batch size causes underfitting. Optimum performance is seen with batch size=200 where training set accuracy is 69% and validation set accuracy is 61% resulting in test accuracy 61.32%."
      ],
      "metadata": {
        "id": "X_Go3bSYD6gE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLJ42_xOOhH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3492d58-f9db-4116-c534-e9d51f48a3ba"
      },
      "source": [
        "#Best Performance\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "LeNet = models.Sequential()\n",
        "\n",
        "LeNet.add(layers.Conv2D(16, (5,5), activation = 'relu', input_shape= (32,32,1)))\n",
        "LeNet.add(layers.AveragePooling2D())\n",
        "\n",
        "LeNet.add(layers.Conv2D(32, (5,5), activation = 'relu'))\n",
        "LeNet.add(layers.AveragePooling2D())\n",
        "LeNet.add(layers.Conv2D(64, (5,5), activation = 'relu'))\n",
        "\n",
        "LeNet.add(layers.Flatten())\n",
        "\n",
        "LeNet.add(layers.Dense(120, activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Dense(84, activation='relu'))\n",
        "\n",
        "LeNet.add(layers.Dense(10, activation='softmax'))\n",
        "LeNet.summary()\n",
        "LeNet.compile(optimizer = 'Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = LeNet.fit(X_train_gray_norm, y_train, epochs=40, batch_size=200, validation_split=0.2, verbose=1)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_42 (Conv2D)          (None, 28, 28, 16)        416       \n",
            "                                                                 \n",
            " average_pooling2d_13 (Avera  (None, 14, 14, 16)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_43 (Conv2D)          (None, 10, 10, 32)        12832     \n",
            "                                                                 \n",
            " average_pooling2d_14 (Avera  (None, 5, 5, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_44 (Conv2D)          (None, 1, 1, 64)          51264     \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 120)               7800      \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 83,326\n",
            "Trainable params: 83,326\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "200/200 [==============================] - 2s 7ms/step - loss: 1.8751 - accuracy: 0.3162 - val_loss: 1.6492 - val_accuracy: 0.3990\n",
            "Epoch 2/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5707 - accuracy: 0.4333 - val_loss: 1.4805 - val_accuracy: 0.4684\n",
            "Epoch 3/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4417 - accuracy: 0.4868 - val_loss: 1.3936 - val_accuracy: 0.5042\n",
            "Epoch 4/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3434 - accuracy: 0.5217 - val_loss: 1.3177 - val_accuracy: 0.5320\n",
            "Epoch 5/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2637 - accuracy: 0.5545 - val_loss: 1.2532 - val_accuracy: 0.5624\n",
            "Epoch 6/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1953 - accuracy: 0.5772 - val_loss: 1.2190 - val_accuracy: 0.5678\n",
            "Epoch 7/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1377 - accuracy: 0.5989 - val_loss: 1.1494 - val_accuracy: 0.5970\n",
            "Epoch 8/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0764 - accuracy: 0.6224 - val_loss: 1.1290 - val_accuracy: 0.6074\n",
            "Epoch 9/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0374 - accuracy: 0.6363 - val_loss: 1.0947 - val_accuracy: 0.6156\n",
            "Epoch 10/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9947 - accuracy: 0.6520 - val_loss: 1.0690 - val_accuracy: 0.6246\n",
            "Epoch 11/40\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.9627 - accuracy: 0.6631 - val_loss: 1.0726 - val_accuracy: 0.6209\n",
            "Epoch 12/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9285 - accuracy: 0.6768 - val_loss: 1.0371 - val_accuracy: 0.6389\n",
            "Epoch 13/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8989 - accuracy: 0.6853 - val_loss: 1.0361 - val_accuracy: 0.6385\n",
            "Epoch 14/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8754 - accuracy: 0.6934 - val_loss: 1.0232 - val_accuracy: 0.6440\n",
            "Epoch 15/40\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.8410 - accuracy: 0.7046 - val_loss: 1.0253 - val_accuracy: 0.6471\n",
            "Epoch 16/40\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.8302 - accuracy: 0.7096 - val_loss: 1.0153 - val_accuracy: 0.6526\n",
            "Epoch 17/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7933 - accuracy: 0.7215 - val_loss: 1.0136 - val_accuracy: 0.6541\n",
            "Epoch 18/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7789 - accuracy: 0.7285 - val_loss: 1.0149 - val_accuracy: 0.6537\n",
            "Epoch 19/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7557 - accuracy: 0.7372 - val_loss: 1.0274 - val_accuracy: 0.6490\n",
            "Epoch 20/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7394 - accuracy: 0.7413 - val_loss: 0.9958 - val_accuracy: 0.6637\n",
            "Epoch 21/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7157 - accuracy: 0.7495 - val_loss: 0.9975 - val_accuracy: 0.6645\n",
            "Epoch 22/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6931 - accuracy: 0.7567 - val_loss: 0.9967 - val_accuracy: 0.6678\n",
            "Epoch 23/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6754 - accuracy: 0.7650 - val_loss: 1.0106 - val_accuracy: 0.6648\n",
            "Epoch 24/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6644 - accuracy: 0.7668 - val_loss: 1.0236 - val_accuracy: 0.6652\n",
            "Epoch 25/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6457 - accuracy: 0.7739 - val_loss: 1.0478 - val_accuracy: 0.6611\n",
            "Epoch 26/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6243 - accuracy: 0.7822 - val_loss: 1.0334 - val_accuracy: 0.6659\n",
            "Epoch 27/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6094 - accuracy: 0.7876 - val_loss: 1.0543 - val_accuracy: 0.6656\n",
            "Epoch 28/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5997 - accuracy: 0.7918 - val_loss: 1.0627 - val_accuracy: 0.6614\n",
            "Epoch 29/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5772 - accuracy: 0.7993 - val_loss: 1.0692 - val_accuracy: 0.6613\n",
            "Epoch 30/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5559 - accuracy: 0.8063 - val_loss: 1.0936 - val_accuracy: 0.6627\n",
            "Epoch 31/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5530 - accuracy: 0.8061 - val_loss: 1.0931 - val_accuracy: 0.6629\n",
            "Epoch 32/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5305 - accuracy: 0.8128 - val_loss: 1.1421 - val_accuracy: 0.6592\n",
            "Epoch 33/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5160 - accuracy: 0.8199 - val_loss: 1.1579 - val_accuracy: 0.6552\n",
            "Epoch 34/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5101 - accuracy: 0.8220 - val_loss: 1.1731 - val_accuracy: 0.6577\n",
            "Epoch 35/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4965 - accuracy: 0.8249 - val_loss: 1.1663 - val_accuracy: 0.6597\n",
            "Epoch 36/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4834 - accuracy: 0.8305 - val_loss: 1.1897 - val_accuracy: 0.6552\n",
            "Epoch 37/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4619 - accuracy: 0.8368 - val_loss: 1.2215 - val_accuracy: 0.6548\n",
            "Epoch 38/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4483 - accuracy: 0.8439 - val_loss: 1.2627 - val_accuracy: 0.6570\n",
            "Epoch 39/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4317 - accuracy: 0.8497 - val_loss: 1.2908 - val_accuracy: 0.6600\n",
            "Epoch 40/40\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4250 - accuracy: 0.8505 - val_loss: 1.2979 - val_accuracy: 0.6561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = LeNet.evaluate(X_test_gray_norm, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sefyoksXHJUu",
        "outputId": "86fa2e28-ee05-48f6-9785-2de1a3a77dac"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 1.3636 - accuracy: 0.6467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Increasing model complexity provides better performance in the validation set. With the default learning rate, and batch size of 200 we increased number of kernels in each layer and added a dense layer to increase the model complexity and it resulted an accuracy of almost 66%. Increasing epoch slightly contributes to the accuracy as well.  \n"
      ],
      "metadata": {
        "id": "jAqhR48hHA4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Equivalent Feed Forward Network\n",
        "from tensorflow import keras\n",
        "# Sequential Model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(32, 32,1)),\n",
        "    keras.layers.Dense(6, activation='relu'),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(120, activation='relu'),\n",
        "    keras.layers.Dense(84, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "history = model.fit(X_train_gray_norm, y_train, epochs=25, batch_size=200)\n",
        "#Test Accuracy\n",
        "score =model.evaluate(X_test_gray_norm, y_test)\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvJ6RNbGH4Uf",
        "outputId": "ad40cb14-08ef-4cdd-e036-512157a4f858"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_54\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_61 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_255 (Dense)           (None, 6)                 6150      \n",
            "                                                                 \n",
            " dense_256 (Dense)           (None, 16)                112       \n",
            "                                                                 \n",
            " dense_257 (Dense)           (None, 120)               2040      \n",
            "                                                                 \n",
            " dense_258 (Dense)           (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_259 (Dense)           (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,316\n",
            "Trainable params: 19,316\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 2.0479 - accuracy: 0.2465\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.9471 - accuracy: 0.2849\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.9251 - accuracy: 0.2952\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.9052 - accuracy: 0.3060\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8848 - accuracy: 0.3165\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8727 - accuracy: 0.3227\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8627 - accuracy: 0.3255\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8562 - accuracy: 0.3289\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8496 - accuracy: 0.3308\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8437 - accuracy: 0.3343\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8380 - accuracy: 0.3381\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8344 - accuracy: 0.3390\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8303 - accuracy: 0.3429\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8262 - accuracy: 0.3445\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8220 - accuracy: 0.3466\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8169 - accuracy: 0.3490\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8136 - accuracy: 0.3502\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8105 - accuracy: 0.3510\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8081 - accuracy: 0.3504\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8062 - accuracy: 0.3518\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.8021 - accuracy: 0.3551\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.7989 - accuracy: 0.3539\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.7973 - accuracy: 0.3542\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.7944 - accuracy: 0.3555\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.7908 - accuracy: 0.3570\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.8560 - accuracy: 0.3361\n",
            "Test accuracy: 0.3361000120639801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Equivalent Feed Forward Network\n",
        "from tensorflow import keras\n",
        "# Sequential Model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(32, 32,1)),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "history = model.fit(X_train_gray_norm, y_train, epochs=25, batch_size=200)\n",
        "#Test Accuracy\n",
        "score =model.evaluate(X_test_gray_norm, y_test)\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzavrFxnaUNB",
        "outputId": "1845418d-cec7-4430-9a9e-6cc3c42da396"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_53\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_60 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_250 (Dense)           (None, 32)                32800     \n",
            "                                                                 \n",
            " dense_251 (Dense)           (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_252 (Dense)           (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_253 (Dense)           (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_254 (Dense)           (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 52,138\n",
            "Trainable params: 52,138\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.9495 - accuracy: 0.3014\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.7802 - accuracy: 0.3727\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.7074 - accuracy: 0.3959\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.6578 - accuracy: 0.4117\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.6182 - accuracy: 0.4252\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.5849 - accuracy: 0.4410\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 1.5566 - accuracy: 0.4476\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 1.5338 - accuracy: 0.4562\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 1.5151 - accuracy: 0.4636\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.4924 - accuracy: 0.4716\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.4727 - accuracy: 0.4795\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.4571 - accuracy: 0.4828\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.4433 - accuracy: 0.4893\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.4253 - accuracy: 0.4939\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.4133 - accuracy: 0.4985\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.4020 - accuracy: 0.5024\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.3875 - accuracy: 0.5093\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.3731 - accuracy: 0.5125\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.3640 - accuracy: 0.5153\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.3486 - accuracy: 0.5236\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.3400 - accuracy: 0.5258\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.3284 - accuracy: 0.5297\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.3209 - accuracy: 0.5323\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.3080 - accuracy: 0.5368\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1.2945 - accuracy: 0.5394\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.7460 - accuracy: 0.4084\n",
            "Test accuracy: 0.4083999991416931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. (a)In this simple feed forward network can provide an accuracy of 33% which is quite well compared to the number of learnable parameters of CNN (almost 3 times). But feed forward network cannot capture the complexity of the given dataset were as LeNet framework can obtain an accuracy of 65% with the same number of layers.\n",
        "(b) We created another feed forward network with same number of layer but higher learnable parameters close to LeNet models no of parameters as here we can see the accuracy increased only 7% (accuracy=40%). So it is not worth it. "
      ],
      "metadata": {
        "id": "QukL0kwVX_0E"
      }
    }
  ]
}